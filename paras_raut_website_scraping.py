# -*- coding: utf-8 -*-
"""Paras_Raut_website_scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/154wsDS2bqMSW4GkXi5HC7N7bTEmr7HnB

# **Name : Paras Raut**
# **Email : parasraut2511@gmail.com**
# **Ph No. : 9356375273**
# **Linked : https://www.linkedin.com/in/paras-raut/**
# **Github : https://github.com/parasraut21**

# How To Setup The file

**Step 1: Open Google Colab**
- Open your web browser and go to Google Colab.

**Step 2: Create a New Notebook**
- Click on "File" in the top-left corner, then select "New Notebook" from the dropdown menu.

**Step 3: Copy and Paste Code Blocks**
- Copy each code block provided and paste them into separate cells in your Colab notebook. To create a new cell, you can either press the "+ CODE" button or use the keyboard shortcut Ctrl + M B.

**Step 4: Run the Code Blocks**
- Run each code block by clicking the play button on the left side of the cell or using the keyboard shortcut Shift + Enter. Make sure to run the code blocks in order, starting from the first one.

**Step 5: Upload the Input File**
- Upload the "Input.xlsx" file by clicking on the file icon in the left sidebar, then selecting "Upload" and choosing your input Excel file.

**Step 6: Verify Output**
- After running the final code block, the output Excel file, positive and negative words files, and stopwords files will be downloaded. You can verify these files in your local machine's download folder.

# Import necessary libraries
"""

import os
import requests
from bs4 import BeautifulSoup
import pandas as pd
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.tokenize import sent_tokenize, word_tokenize
from datetime import datetime
from google.colab import files

"""# Download the necessary NLTK resources"""

nltk.download('punkt')
nltk.download('vader_lexicon')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')

"""# Specify file paths"""

input_file_path = 'Input.xlsx'
output_file_name = f"Output_Data_Structure_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
positive_words_file = "Positive_Words.txt"
negative_words_file = "Negative_Words.txt"
stopwords_file_prefix = "Stopwords_"

"""# Load the input.xlsx file"""

df_input = pd.read_excel(input_file_path)

# Create DataFrames for the output data
columns = ['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE',
           'POLARITY SCORE', 'SUBJECTIVITY SCORE',
           'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS',
           'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE']
df_output = pd.DataFrame(columns=columns)

positive_words = set()
negative_words = set()
stopwords_by_type = {}

df_input

"""# Function to extract article text from a URL"""

def extract_article_text(url):
    try:
        response = requests.get(url)
        response.raise_for_status()


        soup = BeautifulSoup(response.text, 'html.parser')

        article_title = soup.find('h1').text
        article_text = ' '.join([p.text for p in soup.find_all('p')])

        return article_title, article_text

    except Exception as e:
        print(f"Error extracting data from {url}: {e}")
        return None, None

"""# Function to compute text analysis variables"""

def analyze_text(text):

    sentences = sent_tokenize(text)
    words = word_tokenize(text)


    sia = SentimentIntensityAnalyzer()


    pos_score = sia.polarity_scores(text)['pos']
    neg_score = sia.polarity_scores(text)['neg']
    polarity_score = pos_score - neg_score
    subjectivity_score = sia.polarity_scores(text)['compound']


    avg_sentence_length = sum(len(sent.split()) for sent in sentences) / len(sentences)


    complex_words = [word for word in words if len(word) > 2]  # Filter out short words
    percentage_complex_words = (len(complex_words) / len(words)) * 100


    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)


    avg_words_per_sentence = len(words) / len(sentences)

    # Extract positive and negative words
    positive_words.update([word.lower() for word in words if sia.polarity_scores(word)['compound'] >= 0.5])
    negative_words.update([word.lower() for word in words if sia.polarity_scores(word)['compound'] <= -0.5])



    return pos_score, neg_score, polarity_score, subjectivity_score, avg_sentence_length, \
           percentage_complex_words, fog_index, avg_words_per_sentence

"""# Iterate through the DataFrame and perform text analysis for each URL"""

for index, row in df_input.iterrows():
    url_id = row['URL_ID']
    url = row['URL']

    # Extract article text
    title, text = extract_article_text(url)

    if title and text:
        # Perform text analysis
        pos_score, neg_score, polarity_score, subjectivity_score, avg_sentence_length, \
        percentage_complex_words, fog_index, avg_words_per_sentence = analyze_text(text)


        df_output = df_output.append(pd.Series([url_id, url, pos_score, neg_score, polarity_score, subjectivity_score,
                                                avg_sentence_length, percentage_complex_words, fog_index,
                                                avg_words_per_sentence], index=columns), ignore_index=True)

        print(f"Text extracted and analyzed for {url_id}.")

    else:
        print(f"Failed to extract text for {url_id}.")

"""# Save the final DataFrame to the output Excel file"""

df_output.to_excel(output_file_name, index=False)

# Save positive words to a file
with open(positive_words_file, 'w') as file:
    file.write('\n'.join(sorted(positive_words)))

# Save negative words to a file
with open(negative_words_file, 'w') as file:
    file.write('\n'.join(sorted(negative_words)))

"""# Save stopwords to separate files based on types"""

stopwords = nltk.corpus.stopwords.words('english')

for word in stopwords:
    word_type = nltk.pos_tag([word])[0][1]
    stopwords_by_type.setdefault(word_type, []).append(word)

for word_type, stopwords_list in stopwords_by_type.items():
    stopwords_file = f"{stopwords_file_prefix}{word_type}.txt"
    with open(stopwords_file, 'w') as file:
        file.write('\n'.join(sorted(stopwords_list)))

"""# Trigger download for all files"""

files.download(output_file_name)
files.download(positive_words_file)
files.download(negative_words_file)

for word_type in stopwords_by_type:
    stopwords_file = f"{stopwords_file_prefix}{word_type}.txt"
    files.download(stopwords_file)

print("Files downloaded successfully.")